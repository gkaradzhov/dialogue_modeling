{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /Users/georgi/dev/dialogue_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supporting_classifiers.agreement_classifier import *\n",
    "from solution_tracker.simple_sol import solution_tracker, process_raw_to_solution_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "from read_data import read_solution_annotaions, read_wason_dump, read_3_lvl_annotation_file\n",
    "import pandas as pd\n",
    "from featurisers.raw_wason_featuriser import get_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierch_data = read_3_lvl_annotation_file('3lvl_anns.tsv')\n",
    "\n",
    "\n",
    "for a in hierch_data:\n",
    "    a.preprocess_everything(nlp)\n",
    "\n",
    "raw_data = read_wason_dump('data/all/')\n",
    "\n",
    "conversations_to_process = {}\n",
    "for ann in hierch_data:\n",
    "    raw = [r for r in raw_data if r.identifier == ann.identifier][0]\n",
    "    ann.raw_db_conversation = raw.raw_db_conversation\n",
    "    conversations_to_process[ann.identifier] = ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solution_tracker.augment_with_solution import merge_with_solution\n",
    "import plotly.graph_objects as go\n",
    "from featurisers.raw_wason_featuriser import calculate_stats, preprocess_conversation_dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_in_window(conversation, threshold, window_size, bad_cases=False): \n",
    "    output = []\n",
    "    for index, message in enumerate(messages_supervised):\n",
    "        begining = 0 if index-window_size < 0 else index-window_size\n",
    "        running_window = messages_supervised[begining:index]\n",
    "        if len(running_window) == 0:\n",
    "            continue\n",
    "        diff = running_window[-1].annotation['team_performance'] - running_window[0].annotation['team_performance']\n",
    "        if bad_cases:\n",
    "            if diff > threshold:\n",
    "                continue\n",
    "        else:\n",
    "            if diff < threshold:\n",
    "                continue\n",
    "                \n",
    "        inner_window = []\n",
    "        for item in running_window:\n",
    "            inner_window.append({'window_improvement': diff ,'message': item.content, 'annotation': item.annotation,\n",
    "                          'improvement': message.annotation['performance_change']})\n",
    "        output.append(inner_window)\n",
    "      \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_cases = []\n",
    "bad_cases = []\n",
    "for key, conv in conversations_to_process.items():\n",
    "    messages_supervised = merge_with_solution(conv)\n",
    "    output_good = process_in_window(messages_supervised, 0.1, 6)\n",
    "    output_bad = process_in_window(messages_supervised, -0.1, 6, True)\n",
    "\n",
    "    good_cases.extend(output_good)\n",
    "    bad_cases.extend(output_bad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bad_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderred_good = sorted(good_cases, key=lambda k: k[0]['window_improvement'], reverse=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderred_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_orderred(orderd):\n",
    "    for item in orderd:\n",
    "        for message in item:\n",
    "            print(\"{}\\t{}\\t{}\".format(message['message'], message['window_improvement'], message['annotation']))\n",
    "        print('~~~~~~~~~~~~~\\t~~~~~~~~~~\\t~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_orderred(orderred_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_orderred(good_probing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good_probing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderred_good_probing = sorted(good_probing, key=lambda k: k['improvement'], reverse=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderred_good_probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_orderred(bad_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderred_bad = sorted(bad_cases, key=lambda k: k[0]['improvement'], reverse=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_orderred(orderred_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderred_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_probing = [g for g in bad_cases if g['annotation']['type'] == 'Probing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_orderred(bad_probing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderred_bad_probing = sorted(bad_probing, key=lambda k: k['improvement'], reverse=False) \n",
    "orderred_bad_probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
